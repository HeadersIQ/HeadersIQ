{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This script started on: 2024-09-17 12:14:01\n",
      "tpcds.store_sales\n",
      "Database.table: tpcds.store_sales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 12:14:05,085 - INFO - Columns in datasets_df: ['index', 'name', 'database', 'area', 'instances', 'attributes', 'Column', 'Description', 'primary_key', 'foreign_keys']\n",
      "2024-09-17 12:14:05,087 - INFO - Columns: 23\n",
      "2024-09-17 12:14:05,087 - INFO - Descriptions: 23\n",
      "2024-09-17 12:14:05,089 - INFO - Processed table: tpcds.store_sales\n",
      "2024-09-17 12:14:05,090 - INFO - Total rows: 23\n",
      "2024-09-17 12:14:05,199 - INFO - Updated information has been saved to AllColumnsInfo.xlsx\n",
      "2024-09-17 12:14:05,244 - INFO - Updated information has been saved to AllDatasetsInfo.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed for database table: tpcds.store_sales\n",
      "Database extracted: tpcds\n",
      "\n",
      "First few rows of the combined DataFrame:\n",
      "               index         name database    area\n",
      "0  tpcds.store_sales  store_sales    tpcds  Retail\n",
      "1  tpcds.store_sales  store_sales    tpcds  Retail\n",
      "2  tpcds.store_sales  store_sales    tpcds  Retail\n",
      "3  tpcds.store_sales  store_sales    tpcds  Retail\n",
      "4  tpcds.store_sales  store_sales    tpcds  Retail\n",
      "\n",
      "Total rows in combined DataFrame: 23\n",
      "\n",
      "Unique values in 'area' column:\n",
      "['Retail']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Union, Tuple, List, Dict\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def load_excel_file(file_path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        df.columns = df.columns.str.strip().str.replace('\\ufeff', '')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading Excel file {file_path}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def process_table(table_name: str, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    table_info = df[df['index'] == table_name]\n",
    "    \n",
    "    if table_info.empty:\n",
    "        logging.warning(f\"No table found with name: {table_name}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    table_info = table_info.iloc[0]\n",
    "    \n",
    "    database = table_name.split('.')[0] if '.' in table_name else ''\n",
    "    \n",
    "    columns = table_info.get('Column', '').split(', ')\n",
    "    descriptions = table_info.get('Description', '').strip('()').split('), ')\n",
    "    \n",
    "    logging.info(f\"Columns: {len(columns)}\")\n",
    "    logging.info(f\"Descriptions: {len(descriptions)}\")\n",
    "    \n",
    "    if len(columns) != len(descriptions):\n",
    "        logging.warning(f\"Mismatch in column and description counts for {table_name}\")\n",
    "        logging.warning(f\"Columns: {columns}\")\n",
    "        logging.warning(f\"Descriptions: {descriptions}\")\n",
    "        \n",
    "        # Use the shorter length to avoid index errors\n",
    "        min_length = min(len(columns), len(descriptions))\n",
    "        columns = columns[:min_length]\n",
    "        descriptions = descriptions[:min_length]\n",
    "    \n",
    "    data = {\n",
    "        'index': [table_name] * len(columns),\n",
    "        'name': [table_info.get('name', '')] * len(columns),\n",
    "        'database': [database] * len(columns),\n",
    "        'area': [table_info.get('area', '')] * len(columns),\n",
    "        'Original Column': [f\"{i+1}. {desc})\" for i, desc in enumerate(descriptions)],\n",
    "    }\n",
    "    \n",
    "    # Check if all arrays have the same length\n",
    "    if len(set(len(v) for v in data.values())) != 1:\n",
    "        logging.error(f\"Inconsistent lengths in data for {table_name}\")\n",
    "        for k, v in data.items():\n",
    "            logging.error(f\"{k}: {len(v)} items\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    columns_df = pd.DataFrame(data)\n",
    "    \n",
    "    logging.info(f\"Processed table: {table_name}\")\n",
    "    logging.info(f\"Total rows: {len(columns_df)}\")\n",
    "    return columns_df\n",
    "\n",
    "def update_excel_file(new_df: pd.DataFrame, file_name: str):\n",
    "    try:\n",
    "        existing_df = pd.read_excel(file_name)\n",
    "        updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(f\"File {file_name} not found. Creating new file.\")\n",
    "        updated_df = new_df\n",
    "    \n",
    "    updated_df.to_excel(file_name, index=False)\n",
    "    logging.info(f\"Updated information has been saved to {file_name}\")\n",
    "\n",
    "def main():\n",
    "    print(f\"This script started on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    desired_dataset_index = input(\"Insert number for dataset or Database.table for table: \")\n",
    "    print(desired_dataset_index)\n",
    "\n",
    "    try:\n",
    "        desired_dataset_index = int(desired_dataset_index)\n",
    "        print(f\"Dataset#: {desired_dataset_index}\")\n",
    "        DB_or_dataset = \"Dataset\"\n",
    "        datasets_file_path = 'Fortydatasets.xlsx'\n",
    "        analysed_columns_file_path = 'AnalysedColumns.xlsx'\n",
    "    except ValueError:\n",
    "        print(f\"Database.table: {desired_dataset_index}\")\n",
    "        DB_or_dataset = \"DB\"\n",
    "        datasets_file_path = 'all_selected_databases_info.xlsx'\n",
    "        analysed_columns_file_path = 'AnalysedColumnsDB.xlsx'\n",
    "\n",
    "    datasets_df = load_excel_file(datasets_file_path)\n",
    "    analysed_columns_df = load_excel_file(analysed_columns_file_path)\n",
    "\n",
    "    if datasets_df.empty or analysed_columns_df.empty:\n",
    "        logging.error(\"Failed to load necessary Excel files. Exiting.\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Columns in datasets_df: {datasets_df.columns.tolist()}\")\n",
    "\n",
    "    if DB_or_dataset == \"DB\":\n",
    "        if desired_dataset_index not in datasets_df['index'].values:\n",
    "            logging.error(f\"Table {desired_dataset_index} not found in the dataset\")\n",
    "            return\n",
    "        combined_df = process_table(desired_dataset_index, datasets_df)\n",
    "    else:\n",
    "        if desired_dataset_index > len(datasets_df):\n",
    "            logging.error(f\"Dataset index {desired_dataset_index} is out of range\")\n",
    "            return\n",
    "        dataset_info = datasets_df.iloc[desired_dataset_index - 1]\n",
    "        tables = dataset_info['Tables'].split(', ')\n",
    "        combined_df = pd.DataFrame()\n",
    "        for table in tables:\n",
    "            table_df = process_table(f\"{dataset_info['Database']}.{table}\", datasets_df)\n",
    "            combined_df = pd.concat([combined_df, table_df], ignore_index=True)\n",
    "\n",
    "    if not combined_df.empty:\n",
    "        update_excel_file(combined_df, \"AllColumnsInfo.xlsx\")\n",
    "\n",
    "        new_rows = []\n",
    "        if DB_or_dataset == \"DB\":\n",
    "            table_info = datasets_df[datasets_df['index'] == desired_dataset_index].iloc[0]\n",
    "            table_info['database'] = desired_dataset_index.split('.')[0] if '.' in desired_dataset_index else ''\n",
    "            new_rows.append(table_info)\n",
    "        else:\n",
    "            new_rows.append(dataset_info)\n",
    "\n",
    "        update_excel_file(pd.DataFrame(new_rows), \"AllDatasetsInfo.xlsx\")\n",
    "\n",
    "        print(f\"Processing completed for {'dataset' if DB_or_dataset == 'Dataset' else 'database table'}: {desired_dataset_index}\")\n",
    "        print(f\"Database extracted: {combined_df['database'].iloc[0] if not combined_df.empty else 'N/A'}\")\n",
    "        print(\"\\nFirst few rows of the combined DataFrame:\")\n",
    "        print(combined_df[['index', 'name', 'database', 'area']].head())\n",
    "        print(f\"\\nTotal rows in combined DataFrame: {len(combined_df)}\")\n",
    "        print(\"\\nUnique values in 'area' column:\")\n",
    "        print(combined_df['area'].unique())\n",
    "    else:\n",
    "        logging.warning(\"No data processed. Check your input and source files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXIT\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
